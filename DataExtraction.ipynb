{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a7c8617-7a46-4dc0-bc70-4ad497e371a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Exported: aggregated_transaction.csv\n",
      "✅ Exported: aggregated_user.csv\n",
      "✅ Exported: aggregated_insurance.csv\n",
      "✅ Exported: map_transaction.csv\n",
      "✅ Exported: map_user.csv\n",
      "✅ Exported: map_insurance.csv\n",
      "✅ Exported: top_transaction.csv\n",
      "✅ Exported: top_user.csv\n",
      "✅ Exported: top_insurance.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "BASE_DIR = \"pulse/data\"\n",
    "\n",
    "dataframes = {\n",
    "    \"aggregated_transaction\": [],\n",
    "    \"aggregated_user\": [],\n",
    "    \"aggregated_insurance\": [],\n",
    "    \"map_transaction\": [],\n",
    "    \"map_user\": [],\n",
    "    \"map_insurance\": [],\n",
    "    \"top_transaction\": [],\n",
    "    \"top_user\": [],\n",
    "    \"top_insurance\": []\n",
    "}\n",
    "\n",
    "def get_state_year_quarter(filepath):\n",
    "    parts = os.path.normpath(filepath).split(os.sep)\n",
    "    if 'state' in parts:\n",
    "        state_index = parts.index('state')\n",
    "        state = parts[state_index + 1]\n",
    "        year = int(parts[state_index + 2])\n",
    "        quarter = int(parts[state_index + 3].replace('.json', ''))\n",
    "    else:\n",
    "        state = 'India'\n",
    "        year = int(parts[-2])\n",
    "        quarter = int(parts[-1].replace('.json', ''))\n",
    "    return state, year, quarter\n",
    "\n",
    "def process_json_files(base_path, callback):\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        if '.ipynb_checkpoints' in root:\n",
    "            continue\n",
    "        for file in files:\n",
    "            if file.endswith(\".json\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    with open(file_path, 'r') as f:\n",
    "                        data = json.load(f)\n",
    "                    callback(file_path, data)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {file_path}: {e}\")\n",
    "\n",
    "def parse_aggregated_transaction(path, data):\n",
    "    state, year, quarter = get_state_year_quarter(path)\n",
    "    for item in data.get(\"data\", {}).get(\"transactionData\", []):\n",
    "        dataframes[\"aggregated_transaction\"].append({\n",
    "            \"State\": state,\n",
    "            \"Year\": year,\n",
    "            \"Quarter\": quarter,\n",
    "            \"Type\": item[\"name\"],\n",
    "            \"Count\": item[\"paymentInstruments\"][0][\"count\"],\n",
    "            \"Amount\": item[\"paymentInstruments\"][0][\"amount\"]\n",
    "        })\n",
    "\n",
    "def parse_aggregated_user(path, data):\n",
    "    state, year, quarter = get_state_year_quarter(path)\n",
    "    agg = data.get(\"data\", {}).get(\"aggregated\")\n",
    "    if agg:\n",
    "        dataframes[\"aggregated_user\"].append({\n",
    "            \"State\": state,\n",
    "            \"Year\": year,\n",
    "            \"Quarter\": quarter,\n",
    "            \"RegisteredUsers\": agg[\"registeredUsers\"],\n",
    "            \"AppOpens\": agg[\"appOpens\"]\n",
    "        })\n",
    "\n",
    "def parse_aggregated_insurance(path, data):\n",
    "    state, year, quarter = get_state_year_quarter(path)\n",
    "    for item in data.get(\"data\", {}).get(\"transactionData\", []):\n",
    "        dataframes[\"aggregated_insurance\"].append({\n",
    "            \"State\": state,\n",
    "            \"Year\": year,\n",
    "            \"Quarter\": quarter,\n",
    "            \"Count\": item[\"paymentInstruments\"][0][\"count\"],\n",
    "            \"Amount\": item[\"paymentInstruments\"][0][\"amount\"]\n",
    "        })\n",
    "\n",
    "def parse_map_transaction(path, data):\n",
    "    state, year, quarter = get_state_year_quarter(path)\n",
    "    for entry in data.get(\"data\", {}).get(\"hoverDataList\", []):\n",
    "        dataframes[\"map_transaction\"].append({\n",
    "            \"State\": state,\n",
    "            \"Region\": entry[\"name\"],\n",
    "            \"Year\": year,\n",
    "            \"Quarter\": quarter,\n",
    "            \"Count\": entry[\"metric\"][0][\"count\"],\n",
    "            \"Amount\": entry[\"metric\"][0][\"amount\"]\n",
    "        })\n",
    "\n",
    "def parse_map_user(path, data):\n",
    "    state, year, quarter = get_state_year_quarter(path)\n",
    "    for region, info in data.get(\"data\", {}).get(\"hoverData\", {}).items():\n",
    "        dataframes[\"map_user\"].append({\n",
    "            \"State\": state,\n",
    "            \"Region\": region,\n",
    "            \"Year\": year,\n",
    "            \"Quarter\": quarter,\n",
    "            \"RegisteredUsers\": info[\"registeredUsers\"],\n",
    "            \"AppOpens\": info[\"appOpens\"]\n",
    "        })\n",
    "\n",
    "def parse_map_insurance(path, data):\n",
    "    state, year, quarter = get_state_year_quarter(path)\n",
    "    for entry in data.get(\"data\", {}).get(\"hoverDataList\", []):\n",
    "        dataframes[\"map_insurance\"].append({\n",
    "            \"State\": state,\n",
    "            \"Region\": entry[\"name\"],\n",
    "            \"Year\": year,\n",
    "            \"Quarter\": quarter,\n",
    "            \"Count\": entry[\"metric\"][0][\"count\"],\n",
    "            \"Amount\": entry[\"metric\"][0][\"amount\"]\n",
    "        })\n",
    "\n",
    "def parse_top_transaction(path, data):\n",
    "    state, year, quarter = get_state_year_quarter(path)\n",
    "    top_data = data.get(\"data\")\n",
    "    if not top_data:\n",
    "        return\n",
    "    for category in [\"states\", \"districts\", \"pincodes\"]:\n",
    "        items = top_data.get(category)\n",
    "        if not items:\n",
    "            continue\n",
    "        for entry in items:\n",
    "            dataframes[\"top_transaction\"].append({\n",
    "                \"State\": state,\n",
    "                \"Category\": category,\n",
    "                \"Region\": entry.get(\"entityName\"),\n",
    "                \"Year\": year,\n",
    "                \"Quarter\": quarter,\n",
    "                \"Count\": entry[\"metric\"][\"count\"],\n",
    "                \"Amount\": entry[\"metric\"][\"amount\"]\n",
    "            })\n",
    "\n",
    "def parse_top_user(path, data):\n",
    "    state, year, quarter = get_state_year_quarter(path)\n",
    "    top_data = data.get(\"data\")\n",
    "    if not top_data:\n",
    "        return\n",
    "    for category in [\"states\", \"districts\", \"pincodes\"]:\n",
    "        items = top_data.get(category)\n",
    "        if not items:\n",
    "            continue\n",
    "        for entry in items:\n",
    "            dataframes[\"top_user\"].append({\n",
    "                \"State\": state,\n",
    "                \"Category\": category,\n",
    "                \"Region\": entry.get(\"name\"),\n",
    "                \"Year\": year,\n",
    "                \"Quarter\": quarter,\n",
    "                \"RegisteredUsers\": entry[\"registeredUsers\"]\n",
    "            })\n",
    "\n",
    "def parse_top_insurance(path, data):\n",
    "    state, year, quarter = get_state_year_quarter(path)\n",
    "    top_data = data.get(\"data\")\n",
    "    if not top_data:\n",
    "        return\n",
    "    for category in [\"states\", \"districts\", \"pincodes\"]:\n",
    "        items = top_data.get(category)\n",
    "        if not items:\n",
    "            continue\n",
    "        for entry in items:\n",
    "            dataframes[\"top_insurance\"].append({\n",
    "                \"State\": state,\n",
    "                \"Category\": category,\n",
    "                \"Region\": entry.get(\"entityName\"),\n",
    "                \"Year\": year,\n",
    "                \"Quarter\": quarter,\n",
    "                \"Count\": entry[\"metric\"][\"count\"],\n",
    "                \"Amount\": entry[\"metric\"][\"amount\"]\n",
    "            })\n",
    "\n",
    "# Run all processors\n",
    "process_json_files(os.path.join(BASE_DIR, \"aggregated/transaction\"), parse_aggregated_transaction)\n",
    "process_json_files(os.path.join(BASE_DIR, \"aggregated/user\"), parse_aggregated_user)\n",
    "process_json_files(os.path.join(BASE_DIR, \"aggregated/insurance\"), parse_aggregated_insurance)\n",
    "\n",
    "process_json_files(os.path.join(BASE_DIR, \"map/transaction\"), parse_map_transaction)\n",
    "process_json_files(os.path.join(BASE_DIR, \"map/user\"), parse_map_user)\n",
    "process_json_files(os.path.join(BASE_DIR, \"map/insurance\"), parse_map_insurance)\n",
    "\n",
    "process_json_files(os.path.join(BASE_DIR, \"top/transaction\"), parse_top_transaction)\n",
    "process_json_files(os.path.join(BASE_DIR, \"top/user\"), parse_top_user)\n",
    "process_json_files(os.path.join(BASE_DIR, \"top/insurance\"), parse_top_insurance)\n",
    "\n",
    "# Save to CSVs\n",
    "for name, records in dataframes.items():\n",
    "    df = pd.DataFrame(records)\n",
    "    df.to_csv(f\"{name}.csv\", index=False)\n",
    "    print(f\"✅ Exported: {name}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be05a3ba-e574-4c2b-851d-efa2f1e8e93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted aggregated_insurance into phonepe_pulse.db\n",
      "Inserted aggregated_transaction into phonepe_pulse.db\n",
      "Inserted aggregated_user into phonepe_pulse.db\n",
      "Inserted map_insurance into phonepe_pulse.db\n",
      "Inserted map_transaction into phonepe_pulse.db\n",
      "Inserted map_user into phonepe_pulse.db\n",
      "Inserted top_insurance into phonepe_pulse.db\n",
      "Inserted top_transaction into phonepe_pulse.db\n",
      "Inserted top_user into phonepe_pulse.db\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Create SQLite DB\n",
    "conn = sqlite3.connect('phonepe_pulse.db')\n",
    "\n",
    "# Loop through CSVs and insert into DB\n",
    "csv_folder = '.'  # wherever your CSVs are saved\n",
    "for file in os.listdir(csv_folder):\n",
    "    if file.endswith('.csv'):\n",
    "        df = pd.read_csv(file)\n",
    "        table_name = file.replace('.csv', '')\n",
    "        df.to_sql(table_name, conn, if_exists='replace', index=False)\n",
    "        print(f\"Inserted {table_name} into phonepe_pulse.db\")\n",
    "\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051e48ba-f348-448e-bcf1-5578148cc496",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
